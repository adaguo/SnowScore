{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "pd.set_option('max_colwidth',400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.tsv', sep=\"\\t\")\n",
    "test = pd.read_csv('./data/test.tsv', sep=\"\\t\")\n",
    "sub = pd.read_csv('./data/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0         1           1   \n",
       "1         2           1   \n",
       "2         3           1   \n",
       "3         4           1   \n",
       "4         5           1   \n",
       "5         6           1   \n",
       "6         7           1   \n",
       "7         8           1   \n",
       "8         9           1   \n",
       "9        10           1   \n",
       "\n",
       "                                                                                                                                                                                         Phrase  \\\n",
       "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
       "1                                                                                                                 A series of escapades demonstrating the adage that what is good for the goose   \n",
       "2                                                                                                                                                                                      A series   \n",
       "3                                                                                                                                                                                             A   \n",
       "4                                                                                                                                                                                        series   \n",
       "5                                                                                                                          of escapades demonstrating the adage that what is good for the goose   \n",
       "6                                                                                                                                                                                            of   \n",
       "7                                                                                                                             escapades demonstrating the adage that what is good for the goose   \n",
       "8                                                                                                                                                                                     escapades   \n",
       "9                                                                                                                                       demonstrating the adage that what is good for the goose   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining independent</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>This</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining independent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet , introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>quiet</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>, introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and entertaining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective and</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>introspective</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>entertaining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>independent</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth seeking</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>is worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>worth</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>seeking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId  \\\n",
       "63        64           2   \n",
       "64        65           2   \n",
       "65        66           2   \n",
       "66        67           2   \n",
       "67        68           2   \n",
       "68        69           2   \n",
       "69        70           2   \n",
       "70        71           2   \n",
       "71        72           2   \n",
       "72        73           2   \n",
       "73        74           2   \n",
       "74        75           2   \n",
       "75        76           2   \n",
       "76        77           2   \n",
       "77        78           2   \n",
       "78        79           2   \n",
       "79        80           2   \n",
       "80        81           2   \n",
       "\n",
       "                                                                        Phrase  \\\n",
       "63  This quiet , introspective and entertaining independent is worth seeking .   \n",
       "64                     This quiet , introspective and entertaining independent   \n",
       "65                                                                        This   \n",
       "66                          quiet , introspective and entertaining independent   \n",
       "67                                      quiet , introspective and entertaining   \n",
       "68                                                                       quiet   \n",
       "69                                            , introspective and entertaining   \n",
       "70                                              introspective and entertaining   \n",
       "71                                                           introspective and   \n",
       "72                                                               introspective   \n",
       "73                                                                         and   \n",
       "74                                                                entertaining   \n",
       "75                                                                 independent   \n",
       "76                                                          is worth seeking .   \n",
       "77                                                            is worth seeking   \n",
       "78                                                                    is worth   \n",
       "79                                                                       worth   \n",
       "80                                                                     seeking   \n",
       "\n",
       "    Sentiment  \n",
       "63          4  \n",
       "64          3  \n",
       "65          2  \n",
       "66          4  \n",
       "67          3  \n",
       "68          2  \n",
       "69          3  \n",
       "70          3  \n",
       "71          3  \n",
       "72          2  \n",
       "73          2  \n",
       "74          4  \n",
       "75          2  \n",
       "76          3  \n",
       "77          4  \n",
       "78          2  \n",
       "79          2  \n",
       "80          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.SentenceId == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average count of phrases per sentence in train is 18.\n",
      "Average count of phrases per sentence in test is 20.\n"
     ]
    }
   ],
   "source": [
    "print('Average count of phrases per sentence in train is {0:.0f}.'\n",
    "      .format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
    "print('Average count of phrases per sentence in test is {0:.0f}.'\n",
    "      .format(test.groupby('SentenceId')['Phrase'].count().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
      "Number of phrases in test: 66292. Number of sentences in test: 3310.\n"
     ]
    }
   ],
   "source": [
    "print('Number of phrases in train: {}. Number of sentences in train: {}.'\n",
    "      .format(train.shape[0], len(train.SentenceId.unique())))\n",
    "print('Number of phrases in test: {}. Number of sentences in test: {}.'\n",
    "      .format(test.shape[0], len(test.SentenceId.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4) (66292, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length of phrases in train is 7.\n",
      "Average word length of phrases in test is 7.\n"
     ]
    }
   ],
   "source": [
    "print('Average word length of phrases in train is {0:.0f}.'\n",
    "      .format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
    "print('Average word length of phrases in test is {0:.0f}.'\n",
    "      .format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see for example most common trigrams for positive phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text_trigrams = [i for i in ngrams(text.split(), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This quiet , introspective and entertaining independent is worth seeking .'\n",
      " 'quiet , introspective and entertaining independent' 'entertaining' ...\n",
      " 'with universal appeal'\n",
      " 'really do a great job of anchoring the characters in the emotional realities of middle age .'\n",
      " 'a great job of anchoring the characters in the emotional realities of middle age']\n"
     ]
    }
   ],
   "source": [
    "print(train.loc[train.Sentiment == 4, 'Phrase'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 199),\n",
       " (('of', 'the', 'year'), 103),\n",
       " (('.', 'is', 'a'), 87),\n",
       " (('of', 'the', 'best'), 80),\n",
       " (('of', 'the', 'most'), 70),\n",
       " (('is', 'one', 'of'), 50),\n",
       " (('One', 'of', 'the'), 43),\n",
       " ((',', 'and', 'the'), 40),\n",
       " (('the', 'year', \"'s\"), 38),\n",
       " (('It', \"'s\", 'a'), 38),\n",
       " (('it', \"'s\", 'a'), 37),\n",
       " (('.', \"'s\", 'a'), 37),\n",
       " (('a', 'movie', 'that'), 35),\n",
       " (('the', 'edge', 'of'), 34),\n",
       " (('the', 'kind', 'of'), 33),\n",
       " (('of', 'your', 'seat'), 33),\n",
       " (('the', 'film', 'is'), 31),\n",
       " ((',', 'this', 'is'), 31),\n",
       " (('the', 'film', \"'s\"), 31),\n",
       " ((',', 'the', 'film'), 30),\n",
       " (('film', 'that', 'is'), 30),\n",
       " (('as', 'one', 'of'), 30),\n",
       " (('edge', 'of', 'your'), 29),\n",
       " ((',', 'it', \"'s\"), 27),\n",
       " (('a', 'film', 'that'), 27),\n",
       " (('as', 'well', 'as'), 27),\n",
       " ((',', 'funny', ','), 25),\n",
       " ((',', 'but', 'it'), 23),\n",
       " (('films', 'of', 'the'), 23),\n",
       " (('some', 'of', 'the'), 23)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/henry/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((',', 'funny', ','), 33),\n",
       " (('one', 'year', \"'s\"), 28),\n",
       " (('year', \"'s\", 'best'), 26),\n",
       " (('movies', 'ever', 'made'), 19),\n",
       " ((',', 'solid', 'cast'), 19),\n",
       " (('solid', 'cast', ','), 18),\n",
       " ((\"'ve\", 'ever', 'seen'), 16),\n",
       " (('.', 'It', \"'s\"), 16),\n",
       " ((',', 'making', 'one'), 15),\n",
       " (('best', 'films', 'year'), 15),\n",
       " ((',', 'touching', ','), 15),\n",
       " (('exquisite', 'acting', ','), 15),\n",
       " (('acting', ',', 'inventive'), 14),\n",
       " ((',', 'inventive', 'screenplay'), 14),\n",
       " (('jaw-dropping', 'action', 'sequences'), 14),\n",
       " (('good', 'acting', ','), 14),\n",
       " ((\"'s\", 'best', 'films'), 14),\n",
       " (('I', \"'ve\", 'seen'), 14),\n",
       " (('funny', ',', 'even'), 14),\n",
       " (('best', 'war', 'movies'), 13),\n",
       " (('purely', 'enjoyable', 'satisfying'), 13),\n",
       " (('funny', ',', 'touching'), 13),\n",
       " ((',', 'smart', ','), 13),\n",
       " (('inventive', 'screenplay', ','), 13),\n",
       " (('funniest', 'jokes', 'movie'), 13),\n",
       " (('action', 'sequences', ','), 13),\n",
       " (('sequences', ',', 'striking'), 13),\n",
       " ((',', 'striking', 'villains'), 13),\n",
       " (('exquisite', 'motion', 'picture'), 13),\n",
       " (('war', 'movies', 'ever'), 12)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
    "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
    "text_trigrams = [i for i in ngrams(text, 3)]\n",
    "Counter(text_trigrams).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
    "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
    "vectorizer.fit(full_text)\n",
    "train_vectorized = vectorizer.transform(train['Phrase'])\n",
    "test_vectorized = vectorizer.transform(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060,)\n"
     ]
    }
   ],
   "source": [
    "y = train['Sentiment']\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "ovr = OneVsRestClassifier(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.18 s, sys: 639 ms, total: 8.82 s\n",
      "Wall time: 4.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='warn', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ovr.fit(train_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.55%, std 0.07.\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy 56.51%, std 0.68.\n",
      "CPU times: user 55.5 ms, sys: 28 ms, total: 83.5 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = LinearSVC(dual=False)\n",
    "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
    "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(train_vectorized, y);\n",
    "svc.fit(train_vectorized, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "#from keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower = True, filters='')\n",
    "tk.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
    "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./pre-trained/crawl-300d-2M.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_features = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
    "\n",
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    \n",
    "    # early stop\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "    \n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    \n",
    "    # adding drop out for regularization\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    \n",
    "    x_gru = GRU(units, return_sequences = True)(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x1)\n",
    "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    x_lstm = LSTM(units, return_sequences = True)(x1)\n",
    "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "    \n",
    "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
    "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool3_gru, max_pool3_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 162s 1ms/step - loss: 0.3608 - acc: 0.8361 - val_loss: 0.3179 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31792, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 157s 1ms/step - loss: 0.3146 - acc: 0.8568 - val_loss: 0.3121 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.31792 to 0.31215, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 161s 1ms/step - loss: 0.3045 - acc: 0.8609 - val_loss: 0.3085 - val_acc: 0.8561\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31215 to 0.30849, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 163s 1ms/step - loss: 0.2963 - acc: 0.8641 - val_loss: 0.3092 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30849\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 167s 1ms/step - loss: 0.2901 - acc: 0.8673 - val_loss: 0.3081 - val_acc: 0.8574\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30849 to 0.30813, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 170s 1ms/step - loss: 0.2850 - acc: 0.8694 - val_loss: 0.3006 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30813 to 0.30058, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 173s 1ms/step - loss: 0.2810 - acc: 0.8717 - val_loss: 0.3019 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30058\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 178s 1ms/step - loss: 0.2772 - acc: 0.8733 - val_loss: 0.3011 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30058\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 177s 1ms/step - loss: 0.2743 - acc: 0.8753 - val_loss: 0.3014 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30058\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model1(\n",
    "    lr = 1e-3, \n",
    "    lr_d = 1e-10, \n",
    "    units = 64, \n",
    "    spatial_dr = 0.3, \n",
    "    kernel_size1=3, \n",
    "    kernel_size2=2, \n",
    "    dense_units=32, \n",
    "    dr=0.1, \n",
    "    conv_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 327s 2ms/step - loss: 0.3650 - acc: 0.8368 - val_loss: 0.3250 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32495, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 329s 2ms/step - loss: 0.3239 - acc: 0.8528 - val_loss: 0.3145 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32495 to 0.31445, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 317s 2ms/step - loss: 0.3146 - acc: 0.8562 - val_loss: 0.3160 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.31445\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 340s 2ms/step - loss: 0.3073 - acc: 0.8596 - val_loss: 0.3100 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31445 to 0.30995, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 363s 3ms/step - loss: 0.3016 - acc: 0.8622 - val_loss: 0.3049 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30995 to 0.30486, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 341s 2ms/step - loss: 0.2956 - acc: 0.8647 - val_loss: 0.3070 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30486\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 348s 2ms/step - loss: 0.2910 - acc: 0.8673 - val_loss: 0.3039 - val_acc: 0.8582\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30486 to 0.30392, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 327s 2ms/step - loss: 0.2882 - acc: 0.8686 - val_loss: 0.3011 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30392 to 0.30113, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 379s 3ms/step - loss: 0.2842 - acc: 0.8704 - val_loss: 0.2971 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.30113 to 0.29707, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 322s 2ms/step - loss: 0.2820 - acc: 0.8716 - val_loss: 0.2979 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29707\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 341s 2ms/step - loss: 0.2799 - acc: 0.8728 - val_loss: 0.2968 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.29707 to 0.29679, saving model to best_model.hdf5\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 373s 3ms/step - loss: 0.2781 - acc: 0.8737 - val_loss: 0.3002 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.29679\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 354s 3ms/step - loss: 0.2759 - acc: 0.8746 - val_loss: 0.3005 - val_acc: 0.8606\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.29679\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 337s 2ms/step - loss: 0.2748 - acc: 0.8750 - val_loss: 0.3014 - val_acc: 0.8618\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.29679\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model1(\n",
    "    lr = 1e-3,\n",
    "    lr_d = 1e-10,\n",
    "    units = 128,\n",
    "    spatial_dr = 0.5,\n",
    "    kernel_size1 = 3,\n",
    "    kernel_size2 = 2,\n",
    "    dense_units = 64,\n",
    "    dr = 0.2,\n",
    "    conv_size = 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    file_path = \"best_model.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                  save_best_only = True, mode = \"min\")\n",
    "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "    inp = Input(shape = (max_len,))\n",
    "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    \n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "    \n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "    \n",
    "    \n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "    \n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
    "    x = Dense(5, activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
    "                        verbose = 1, callbacks = [check_point, early_stop])\n",
    "    model = load_model(file_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 342s 2ms/step - loss: 0.5248 - acc: 0.7325 - val_loss: 0.3933 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39329, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 1547s 11ms/step - loss: 0.3943 - acc: 0.8300 - val_loss: 0.3454 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39329 to 0.34542, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 305s 2ms/step - loss: 0.3621 - acc: 0.8417 - val_loss: 0.3296 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34542 to 0.32956, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 17880s 127ms/step - loss: 0.3477 - acc: 0.8460 - val_loss: 0.3246 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32956 to 0.32457, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 9620s 68ms/step - loss: 0.3393 - acc: 0.8487 - val_loss: 0.3193 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.32457 to 0.31930, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 409s 3ms/step - loss: 0.3329 - acc: 0.8509 - val_loss: 0.3175 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31930 to 0.31746, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 652s 5ms/step - loss: 0.3281 - acc: 0.8522 - val_loss: 0.3153 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31746 to 0.31530, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 310s 2ms/step - loss: 0.3243 - acc: 0.8533 - val_loss: 0.3126 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31530 to 0.31260, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 331s 2ms/step - loss: 0.3211 - acc: 0.8552 - val_loss: 0.3122 - val_acc: 0.8549\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.31260 to 0.31216, saving model to best_model.hdf5\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 328s 2ms/step - loss: 0.3189 - acc: 0.8556 - val_loss: 0.3116 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.31216 to 0.31163, saving model to best_model.hdf5\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 316s 2ms/step - loss: 0.3162 - acc: 0.8566 - val_loss: 0.3096 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.31163 to 0.30956, saving model to best_model.hdf5\n",
      "Epoch 12/20\n",
      "140454/140454 [==============================] - 342s 2ms/step - loss: 0.3135 - acc: 0.8575 - val_loss: 0.3094 - val_acc: 0.8566\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.30956 to 0.30942, saving model to best_model.hdf5\n",
      "Epoch 13/20\n",
      "140454/140454 [==============================] - 339s 2ms/step - loss: 0.3113 - acc: 0.8582 - val_loss: 0.3080 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.30942 to 0.30804, saving model to best_model.hdf5\n",
      "Epoch 14/20\n",
      "140454/140454 [==============================] - 350s 2ms/step - loss: 0.3097 - acc: 0.8586 - val_loss: 0.3080 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.30804 to 0.30802, saving model to best_model.hdf5\n",
      "Epoch 15/20\n",
      "140454/140454 [==============================] - 311s 2ms/step - loss: 0.3081 - acc: 0.8589 - val_loss: 0.3074 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.30802 to 0.30744, saving model to best_model.hdf5\n",
      "Epoch 16/20\n",
      "140454/140454 [==============================] - 302s 2ms/step - loss: 0.3062 - acc: 0.8600 - val_loss: 0.3071 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.30744 to 0.30712, saving model to best_model.hdf5\n",
      "Epoch 17/20\n",
      "140454/140454 [==============================] - 2696s 19ms/step - loss: 0.3042 - acc: 0.8613 - val_loss: 0.3058 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.30712 to 0.30577, saving model to best_model.hdf5\n",
      "Epoch 18/20\n",
      "140454/140454 [==============================] - 302s 2ms/step - loss: 0.3027 - acc: 0.8618 - val_loss: 0.3084 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30577\n",
      "Epoch 19/20\n",
      "140454/140454 [==============================] - 4956s 35ms/step - loss: 0.3010 - acc: 0.8622 - val_loss: 0.3094 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30577\n",
      "Epoch 20/20\n",
      "140454/140454 [==============================] - 8786s 63ms/step - loss: 0.2999 - acc: 0.8631 - val_loss: 0.3086 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30577\n"
     ]
    }
   ],
   "source": [
    "model3 = build_model2(\n",
    "    lr = 1e-4, \n",
    "    lr_d = 0, \n",
    "    units = 64, \n",
    "    spatial_dr = 0.5, \n",
    "    kernel_size1=4, \n",
    "    kernel_size2=3, \n",
    "    dense_units=32, \n",
    "    dr=0.1, \n",
    "    conv_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 8773s 62ms/step - loss: 0.3801 - acc: 0.8304 - val_loss: 0.3210 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32095, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 2121s 15ms/step - loss: 0.3270 - acc: 0.8529 - val_loss: 0.3141 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32095 to 0.31414, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 341s 2ms/step - loss: 0.3159 - acc: 0.8566 - val_loss: 0.3087 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.31414 to 0.30865, saving model to best_model.hdf5\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 343s 2ms/step - loss: 0.3094 - acc: 0.8592 - val_loss: 0.3069 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30865 to 0.30689, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 351s 3ms/step - loss: 0.3019 - acc: 0.8621 - val_loss: 0.3101 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30689\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 362s 3ms/step - loss: 0.2968 - acc: 0.8640 - val_loss: 0.3011 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30689 to 0.30106, saving model to best_model.hdf5\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 374s 3ms/step - loss: 0.2925 - acc: 0.8659 - val_loss: 0.2999 - val_acc: 0.8612\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30106 to 0.29995, saving model to best_model.hdf5\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 388s 3ms/step - loss: 0.2880 - acc: 0.8686 - val_loss: 0.3032 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.29995\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 399s 3ms/step - loss: 0.2852 - acc: 0.8698 - val_loss: 0.3011 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.29995\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 409s 3ms/step - loss: 0.2821 - acc: 0.8708 - val_loss: 0.3009 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.29995\n"
     ]
    }
   ],
   "source": [
    "model4 = build_model2(\n",
    "    lr = 1e-3, \n",
    "    lr_d = 0, \n",
    "    units = 64, \n",
    "    spatial_dr = 0.5, \n",
    "    kernel_size1=3, \n",
    "    kernel_size2=3, \n",
    "    dense_units=64, \n",
    "    dr=0.3, \n",
    "    conv_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140454 samples, validate on 15606 samples\n",
      "Epoch 1/20\n",
      "140454/140454 [==============================] - 407s 3ms/step - loss: 0.3642 - acc: 0.8366 - val_loss: 0.3173 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.31726, saving model to best_model.hdf5\n",
      "Epoch 2/20\n",
      "140454/140454 [==============================] - 374s 3ms/step - loss: 0.3177 - acc: 0.8561 - val_loss: 0.3097 - val_acc: 0.8560\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.31726 to 0.30967, saving model to best_model.hdf5\n",
      "Epoch 3/20\n",
      "140454/140454 [==============================] - 378s 3ms/step - loss: 0.3049 - acc: 0.8607 - val_loss: 0.3107 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30967\n",
      "Epoch 4/20\n",
      "140454/140454 [==============================] - 394s 3ms/step - loss: 0.2959 - acc: 0.8642 - val_loss: 0.3076 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30967 to 0.30764, saving model to best_model.hdf5\n",
      "Epoch 5/20\n",
      "140454/140454 [==============================] - 397s 3ms/step - loss: 0.2886 - acc: 0.8677 - val_loss: 0.3030 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30764 to 0.30304, saving model to best_model.hdf5\n",
      "Epoch 6/20\n",
      "140454/140454 [==============================] - 397s 3ms/step - loss: 0.2825 - acc: 0.8707 - val_loss: 0.3054 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30304\n",
      "Epoch 7/20\n",
      "140454/140454 [==============================] - 2418s 17ms/step - loss: 0.2777 - acc: 0.8728 - val_loss: 0.3040 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30304\n",
      "Epoch 8/20\n",
      "140454/140454 [==============================] - 365s 3ms/step - loss: 0.2739 - acc: 0.8750 - val_loss: 0.3027 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30304 to 0.30267, saving model to best_model.hdf5\n",
      "Epoch 9/20\n",
      "140454/140454 [==============================] - 371s 3ms/step - loss: 0.2695 - acc: 0.8777 - val_loss: 0.3050 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30267\n",
      "Epoch 10/20\n",
      "140454/140454 [==============================] - 372s 3ms/step - loss: 0.2660 - acc: 0.8790 - val_loss: 0.3059 - val_acc: 0.8610\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.30267\n",
      "Epoch 11/20\n",
      "140454/140454 [==============================] - 381s 3ms/step - loss: 0.2637 - acc: 0.8811 - val_loss: 0.3053 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.30267\n"
     ]
    }
   ],
   "source": [
    "model5 = build_model2(\n",
    "    lr = 1e-3, \n",
    "    lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66292/66292 [==============================] - 28s 417us/step\n",
      "66292/66292 [==============================] - 51s 766us/step\n",
      "66292/66292 [==============================] - 68s 1ms/step\n",
      "66292/66292 [==============================] - 67s 1ms/step\n",
      "66292/66292 [==============================] - 82s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred = pred1\n",
    "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred2\n",
    "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred3\n",
    "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred4\n",
    "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "pred += pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
    "sub['Sentiment'] = predictions\n",
    "sub.to_csv(\"blend.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.5658092e-03, 1.6220912e-01, 4.9259537e-01, 3.4582603e-01,\n",
       "        1.4740527e-02],\n",
       "       [5.1175356e-03, 1.6310227e-01, 5.4971617e-01, 2.9768705e-01,\n",
       "        8.9170635e-03],\n",
       "       [2.0127296e-03, 4.4901222e-02, 8.8044274e-01, 7.1183890e-02,\n",
       "        1.0857284e-03],\n",
       "       ...,\n",
       "       [4.7925979e-02, 5.8758914e-01, 3.5295570e-01, 8.7141693e-03,\n",
       "        3.3050776e-05],\n",
       "       [7.6015413e-02, 6.2955600e-01, 2.7752274e-01, 6.1512887e-03,\n",
       "        2.4378300e-05],\n",
       "       [4.6920449e-02, 6.2212574e-01, 3.2350203e-01, 6.4088628e-03,\n",
       "        1.7316592e-05]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('./data/sampleSubmission.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "sub1['predict'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sub1['Sentiment'] == sub1['predict']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for ele in a:\n",
    "    if ele:\n",
    "        acc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38414"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
